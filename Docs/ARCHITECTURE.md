# SeungJz ì• í”Œë¦¬ì¼€ì´ì…˜ ì•„í‚¤í…ì²˜

## ì‹œìŠ¤í…œ ê°œìš”

3-Tier ì•„í‚¤í…ì²˜ ê¸°ë°˜ì˜ ëª¨ë°”ì¼ ìš°ì„  í•œêµ­ì–´ í•™ìŠµ í”Œë«í¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Mobile Web    â”‚  React SPA
â”‚   (Frontend)    â”‚  Web Speech API
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTPS/REST
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spring Boot    â”‚  Business Logic
â”‚   (Backend)     â”‚  Video Streaming
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ JDBC
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚  Persistent Storage
â”‚   (Database)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ML Services    â”‚  KoSpeech STT
â”‚  (Python/Flask) â”‚  DeepFace (Emotion)
â”‚                 â”‚  GPT API
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ê³„ì¸µë³„ ìƒì„¸ ì„¤ê³„

### 1. Frontend Layer (React)

#### ì£¼ìš” ì±…ì„
- ëª¨ë°”ì¼ ìµœì í™” UI/UX
- ë¹„ë””ì˜¤ ì¬ìƒ ë° ë™ê¸°í™”
- 1-2ë‹¨ê³„ í´ë¼ì´ì–¸íŠ¸ ì¸¡ ìŒì„±ì¸ì‹
- í•™ìŠµ ì§„ë„ ì‹œê°í™”

#### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”œâ”€â”€ SocialLogin.jsx        # ì†Œì…œ ë¡œê·¸ì¸ UI
â”‚   â”‚   â””â”€â”€ AuthCallback.jsx       # OAuth ì½œë°± ì²˜ë¦¬
â”‚   â”œâ”€â”€ learning/
â”‚   â”‚   â”œâ”€â”€ LectureList.jsx        # í•™ìŠµ ì»¨í…ì¸  ëª©ë¡
â”‚   â”‚   â”œâ”€â”€ VideoPlayer.jsx        # ë¹„ë””ì˜¤ ì¬ìƒê¸° (Video.js)
â”‚   â”‚   â”œâ”€â”€ QuestionOverlay.jsx    # ì§ˆë¬¸ ì˜¤ë²„ë ˆì´
â”‚   â”‚   â”œâ”€â”€ VoiceInput.jsx         # ìŒì„± ì…ë ¥ (Web Speech API)
â”‚   â”‚   â””â”€â”€ ResultCard.jsx         # ê²°ê³¼ ë° í”¼ë“œë°±
â”‚   â”œâ”€â”€ interview/
â”‚   â”‚   â”œâ”€â”€ PromptInput.jsx        # 3ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ì…ë ¥
â”‚   â”‚   â”œâ”€â”€ RealTimeQA.jsx         # ì‹¤ì‹œê°„ ì§ˆë¬¸/ë‹µë³€
â”‚   â”‚   â”œâ”€â”€ VideoRecorder.jsx      # 3ë‹¨ê³„ ë¹„ë””ì˜¤ ë…¹í™”
â”‚   â”‚   â”œâ”€â”€ EmotionOverlay.jsx     # ì‹¤ì‹œê°„ ê°ì • í‘œì‹œ
â”‚   â”‚   â””â”€â”€ TimelineFeedback.jsx   # íƒ€ì„ë¼ì¸ë³„ í”¼ë“œë°±
â”‚   â””â”€â”€ review/
â”‚       â””â”€â”€ LearningReview.jsx     # í•™ìŠµ ë³µê¸°
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api.js                     # Axios ê¸°ë°˜ API í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ speechRecognition.js       # ìŒì„±ì¸ì‹ (ì„œë²„ë¡œ ì „ì†¡)
â”‚   â”œâ”€â”€ videoSync.js               # ë¹„ë””ì˜¤-ì§ˆë¬¸ ë™ê¸°í™”
â”‚   â””â”€â”€ videoCapture.js            # 3ë‹¨ê³„ ë¹„ë””ì˜¤ í”„ë ˆì„ ìº¡ì²˜
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useAuth.js                 # ì¸ì¦ ìƒíƒœ ê´€ë¦¬
â”‚   â”œâ”€â”€ useSpeechRecognition.js    # ìŒì„±ì¸ì‹ í›…
â”‚   â””â”€â”€ useLearning.js             # í•™ìŠµ ìƒíƒœ ê´€ë¦¬
â””â”€â”€ store/
    â”œâ”€â”€ authSlice.js               # Redux: ì‚¬ìš©ì ì¸ì¦
    â””â”€â”€ learningSlice.js           # Redux: í•™ìŠµ ì§„ë„
```

#### 3ë‹¨ê³„ ë¹„ë””ì˜¤ ë…¹í™” ë° ê°ì • ë¶„ì„

**VideoRecorder ì»´í¬ë„ŒíŠ¸**:
```javascript
// services/videoCapture.js
import { useRef, useState } from 'react';

export const useVideoCapture = () => {
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecorderRef = useRef(null);
  const videoStreamRef = useRef(null);

  const startRecording = async () => {
    // ì¹´ë©”ë¼ + ë§ˆì´í¬ ì ‘ê·¼
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { width: 1280, height: 720, facingMode: 'user' },
      audio: true
    });

    videoStreamRef.current = stream;

    // MediaRecorderë¡œ ì „ì²´ ì„¸ì…˜ ë…¹í™”
    const recorder = new MediaRecorder(stream, {
      mimeType: 'video/webm;codecs=vp9'
    });

    mediaRecorderRef.current = recorder;

    // í”„ë ˆì„ ìº¡ì²˜ (1ì´ˆë§ˆë‹¤ DeepFace ë¶„ì„ìš©)
    const canvas = document.createElement('canvas');
    const video = document.createElement('video');
    video.srcObject = stream;

    const captureInterval = setInterval(() => {
      canvas.getContext('2d').drawImage(video, 0, 0);
      canvas.toBlob(blob => {
        // ì„œë²„ë¡œ í”„ë ˆì„ ì „ì†¡ (ê°ì • ë¶„ì„)
        sendFrameForAnalysis(blob);
      }, 'image/jpeg', 0.8);
    }, 1000);  // 1ì´ˆë§ˆë‹¤

    recorder.start();
    setIsRecording(true);
  };

  const sendFrameForAnalysis = async (frameBlob) => {
    const formData = new FormData();
    formData.append('frame', frameBlob);

    const response = await fetch('/api/emotion/analyze-frame', {
      method: 'POST',
      body: formData
    });

    const result = await response.json();
    // ì‹¤ì‹œê°„ ê°ì • í‘œì‹œ ì—…ë°ì´íŠ¸
    return result;
  };

  return { startRecording, isRecording };
};
```

**ì¥ì **:
- ì‹¤ì‹œê°„ ê°ì • í”¼ë“œë°±
- ì „ì²´ ì„¸ì…˜ ë¹„ë””ì˜¤ ì €ì¥ (ë³µê¸°ìš©)
- 1ì´ˆ ë‹¨ìœ„ ê°ì • íƒ€ì„ë¼ì¸

---

### 2. Backend Layer (Spring Boot)

#### ì£¼ìš” ì±…ì„
- RESTful API ì œê³µ
- ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° (Adaptive Bitrate)
- Lecture ë° Component ê´€ë¦¬
- 3ë‹¨ê³„ ê³ ê¸‰ ìŒì„±ì¸ì‹ ì²˜ë¦¬ (KoSpeech ì—°ë™)
- AI ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± ë° í”¼ë“œë°±

#### íŒ¨í‚¤ì§€ êµ¬ì¡°

```
com.seungjz.edutech/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ SecurityConfig.java        # Spring Security + OAuth2
â”‚   â”œâ”€â”€ WebConfig.java             # CORS, Interceptor
â”‚   â””â”€â”€ S3Config.java              # AWS S3 ì„¤ì •
â”œâ”€â”€ controller/
â”‚   â”œâ”€â”€ AuthController.java        # ë¡œê·¸ì¸/íšŒì›ê°€ì…
â”‚   â”œâ”€â”€ LectureController.java     # í•™ìŠµ ì»¨í…ì¸  CRUD
â”‚   â”œâ”€â”€ LearningController.java    # í•™ìŠµ ì§„í–‰ API
â”‚   â””â”€â”€ InterviewController.java   # 3ë‹¨ê³„ ì‹¤ì‹œê°„ ë©´ì ‘
â”œâ”€â”€ service/
â”‚   â”œâ”€â”€ LectureService.java        # Lecture ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”œâ”€â”€ VideoStreamService.java    # ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°
â”‚   â”œâ”€â”€ SpeechService.java         # ìŒì„±ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
â”‚   â”œâ”€â”€ FeedbackService.java       # AI í”¼ë“œë°± ìƒì„±
â”‚   â””â”€â”€ ProgressService.java       # í•™ìŠµ ì§„ë„ ê´€ë¦¬
â”œâ”€â”€ repository/
â”‚   â”œâ”€â”€ UserRepository.java
â”‚   â”œâ”€â”€ LectureRepository.java
â”‚   â”œâ”€â”€ ComponentRepository.java
â”‚   â””â”€â”€ ProgressRepository.java
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ User.java
â”‚   â”œâ”€â”€ Lecture.java               # í•™ìŠµ ì»¨í…ì¸ 
â”‚   â”œâ”€â”€ Component.java             # Lecture êµ¬ì„±ìš”ì†Œ
â”‚   â”œâ”€â”€ Progress.java              # ì‚¬ìš©ì ì§„ë„
â”‚   â””â”€â”€ Feedback.java              # AI í”¼ë“œë°±
â””â”€â”€ external/
    â”œâ”€â”€ KoSpeechClient.java        # Python ML ì„œë¹„ìŠ¤ ì—°ë™
    â””â”€â”€ OpenAIClient.java          # GPT API ì—°ë™
```

#### ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ì•„í‚¤í…ì²˜

```java
@RestController
@RequestMapping("/api/videos")
public class VideoStreamController {

    // HLS (HTTP Live Streaming) ì§€ì›
    @GetMapping("/{lectureId}/stream")
    public ResponseEntity<Resource> streamVideo(
        @PathVariable Long lectureId,
        @RequestHeader(value = "Range", required = false) String range
    ) {
        // AWS S3ì—ì„œ ë¹„ë””ì˜¤ ê°€ì ¸ì˜¤ê¸°
        VideoMetadata metadata = videoService.getMetadata(lectureId);

        // Range ìš”ì²­ ì²˜ë¦¬ (Seek ì§€ì›)
        if (range != null) {
            return handleRangeRequest(metadata, range);
        }

        return ResponseEntity.ok()
            .contentType(MediaType.parseMediaType("video/mp4"))
            .body(videoService.getVideoResource(lectureId));
    }
}
```

**ë¹„ë””ì˜¤ ì €ì¥ ì „ëµ**:
- AWS S3 + CloudFront CDN
- ë‹¤ì¤‘ í•´ìƒë„ ì¸ì½”ë”© (360p, 720p, 1080p)
- HLS/DASH í”„ë¡œí† ì½œ ì§€ì›

#### Lecture Component ì‹œìŠ¤í…œ

```java
@Entity
public class Lecture {
    @Id @GeneratedValue
    private Long id;

    private String title;
    private Integer level; // 1, 2, 3ë‹¨ê³„

    @OneToMany(mappedBy = "lecture", cascade = CascadeType.ALL)
    private List<Component> components;
}

@Entity
public class Component {
    @Id @GeneratedValue
    private Long id;

    @ManyToOne
    private Lecture lecture;

    @Enumerated(EnumType.STRING)
    private ComponentType type; // VIDEO, QUESTION, ANSWER_CHOICE

    private Integer sequence; // ìˆœì„œ
    private String content; // JSON í˜•íƒœì˜ ì»¨í…ì¸ 

    // VIDEO: { "videoUrl": "...", "startTime": 0, "endTime": 30 }
    // QUESTION: { "text": "...", "triggerTime": 25 }
    // ANSWER_CHOICE: { "choices": [...], "correctIndex": 0 }
}
```

**Component ì‹¤í–‰ íë¦„**:
1. í´ë¼ì´ì–¸íŠ¸ê°€ `/api/lectures/{id}/components` ìš”ì²­
2. ì„œë²„ê°€ sequence ìˆœì„œëŒ€ë¡œ Component ë°˜í™˜
3. VIDEO â†’ QUESTION â†’ (ì‚¬ìš©ì ë‹µë³€) â†’ ê²€ì¦ â†’ ë‹¤ìŒ Component

#### 3ë‹¨ê³„ ì‹¤ì‹œê°„ ë©´ì ‘ ì²˜ë¦¬

```java
@Service
public class InterviewService {

    @Autowired
    private KoSpeechClient koSpeechClient;

    @Autowired
    private OpenAIClient openAIClient;

    // WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ ì²˜ë¦¬
    public void handleInterviewSession(String sessionId, String userPrompt) {
        // 1. GPTë¡œ ì´ˆê¸° ì§ˆë¬¸ ìƒì„± (5W1H ê¸°ë°˜)
        List<String> questions = openAIClient.generateQuestions(userPrompt);

        // 2. 3ë¶„ê°„ ì§ˆë¬¸-ë‹µë³€ ë°˜ë³µ
        for (int i = 0; i < questions.size(); i++) {
            webSocketService.sendQuestion(sessionId, questions.get(i));

            // ì‚¬ìš©ì ìŒì„± ìˆ˜ì‹  (WebSocket)
            byte[] audioData = webSocketService.receiveAudio(sessionId);

            // 3. KoSpeechë¡œ ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜
            String transcript = koSpeechClient.transcribe(audioData);

            // 4. GPTë¡œ ë‹µë³€ ë¶„ì„ ë° ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±
            AnalysisResult analysis = openAIClient.analyzeAnswer(
                questions.get(i), transcript
            );

            // í‚¤ì›Œë“œ ëˆ„ë½, ë¬¸ë§¥ ì–´ìƒ‰í•¨ íƒì§€
            if (analysis.hasMissingKeywords() || analysis.isAwkward()) {
                feedbackList.add(createTimedFeedback(analysis));
            }

            // ì—°ê³„ ì§ˆë¬¸ ìƒì„±
            questions.add(openAIClient.generateFollowUp(transcript));
        }

        // 5. ìµœì¢… í”¼ë“œë°± ìƒì„± (MP4 íƒ€ì„ë¼ì¸ ë§í¬)
        return generateTimelineFeedback(feedbackList);
    }
}
```

---

### 3. Database Layer (PostgreSQL)

#### ìŠ¤í‚¤ë§ˆ ì„¤ê³„

```sql
-- ì‚¬ìš©ì ê´€ë¦¬
CREATE TABLE users (
    id BIGSERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    provider VARCHAR(50) NOT NULL, -- GOOGLE, KAKAO
    provider_id VARCHAR(255) NOT NULL,
    nickname VARCHAR(100),
    profile_image_url TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- í•™ìŠµ ì»¨í…ì¸ 
CREATE TABLE lectures (
    id BIGSERIAL PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    description TEXT,
    level INTEGER NOT NULL, -- 1, 2, 3
    thumbnail_url TEXT,
    video_url TEXT, -- S3 URL
    duration INTEGER, -- ì´ˆ ë‹¨ìœ„
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Lecture êµ¬ì„±ìš”ì†Œ
CREATE TABLE components (
    id BIGSERIAL PRIMARY KEY,
    lecture_id BIGINT REFERENCES lectures(id) ON DELETE CASCADE,
    type VARCHAR(50) NOT NULL, -- VIDEO, QUESTION, ANSWER_CHOICE
    sequence INTEGER NOT NULL, -- ìˆœì„œ
    content JSONB NOT NULL, -- ìœ ì—°í•œ ì»¨í…ì¸  ì €ì¥
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(lecture_id, sequence)
);

-- í•™ìŠµ ì§„ë„
CREATE TABLE progress (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT REFERENCES users(id) ON DELETE CASCADE,
    lecture_id BIGINT REFERENCES lectures(id) ON DELETE CASCADE,
    completed_components JSONB DEFAULT '[]', -- [1, 2, 3]
    score INTEGER, -- ì •ë‹µë¥ 
    last_accessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_completed BOOLEAN DEFAULT FALSE,
    UNIQUE(user_id, lecture_id)
);

-- 3ë‹¨ê³„ ë©´ì ‘ ì„¸ì…˜
CREATE TABLE interview_sessions (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT REFERENCES users(id) ON DELETE CASCADE,
    prompt TEXT NOT NULL, -- ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸
    duration INTEGER DEFAULT 180, -- 3ë¶„
    video_recording_url TEXT, -- ë…¹í™” ì˜ìƒ URL
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- AI í”¼ë“œë°±
CREATE TABLE feedback (
    id BIGSERIAL PRIMARY KEY,
    session_id BIGINT REFERENCES interview_sessions(id) ON DELETE CASCADE,
    timeline_sec INTEGER, -- íƒ€ì„ë¼ì¸ ìœ„ì¹˜ (ì´ˆ)
    feedback_type VARCHAR(50), -- MISSING_KEYWORD, AWKWARD_CONTEXT
    content TEXT, -- í”¼ë“œë°± ë‚´ìš©
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ì¸ë±ìŠ¤ ìƒì„±
CREATE INDEX idx_progress_user ON progress(user_id);
CREATE INDEX idx_components_lecture ON components(lecture_id, sequence);
CREATE INDEX idx_feedback_session ON feedback(session_id);
```

---

### 4. ML Services Layer (Python)

ìŒì„±ì¸ì‹ ë° AI ë¶„ì„ì„ ìœ„í•œ ë³„ë„ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

#### êµ¬ì¡°

```
ml-services/
â”œâ”€â”€ kospeech/
â”‚   â”œâ”€â”€ app.py                 # Flask API ì„œë²„
â”‚   â”œâ”€â”€ model_loader.py        # KoSpeech ëª¨ë¸ ë¡œë“œ
â”‚   â””â”€â”€ transcribe.py          # ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜
â”œâ”€â”€ feedback/
â”‚   â”œâ”€â”€ analyzer.py            # ë¬¸ë§¥ ë¶„ì„
â”‚   â””â”€â”€ keyword_extractor.py   # í‚¤ì›Œë“œ ì¶”ì¶œ
â””â”€â”€ requirements.txt
```

#### KoSpeech API ì„œë²„

```python
# ml-services/kospeech/app.py
from flask import Flask, request, jsonify
from model_loader import load_kospeech_model

app = Flask(__name__)
model = load_kospeech_model()

@app.route('/transcribe', methods=['POST'])
def transcribe():
    """
    ìŒì„± íŒŒì¼ì„ ë°›ì•„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    audio_file = request.files['audio']

    # KoSpeech ëª¨ë¸ë¡œ ë³€í™˜
    transcript = model.transcribe(audio_file)

    return jsonify({
        'transcript': transcript,
        'confidence': 0.95
    })

@app.route('/analyze', methods=['POST'])
def analyze():
    """
    ë‹µë³€ ë¶„ì„: í‚¤ì›Œë“œ ëˆ„ë½, ë¬¸ë§¥ ì–´ìƒ‰í•¨ íƒì§€
    """
    data = request.json
    question = data['question']
    answer = data['answer']

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    expected_keywords = extract_keywords(question)
    user_keywords = extract_keywords(answer)
    missing_keywords = set(expected_keywords) - set(user_keywords)

    # ë¬¸ë§¥ ë¶„ì„ (GPT API í™œìš©)
    context_score = analyze_context(answer)

    return jsonify({
        'missing_keywords': list(missing_keywords),
        'context_score': context_score,
        'is_awkward': context_score < 0.7
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

---

## ì£¼ìš” ê¸°ëŠ¥ë³„ í”Œë¡œìš°

### 1ë‹¨ê³„/2ë‹¨ê³„ í•™ìŠµ í”Œë¡œìš°

```
[ì‚¬ìš©ì] â†’ [Frontend]
   â”‚
   â”œâ”€ GET /api/lectures?level=1 â”€â†’ [Backend]
   â”‚                                    â”‚
   â”‚                                    â”œâ”€ Query Lectures
   â”‚                                    â””â”€ Return List
   â”‚
   â”œâ”€ GET /api/lectures/{id}/components
   â”‚                                    â”‚
   â”‚                                    â””â”€ Return [VIDEO, QUESTION, ...]
   â”‚
   â”œâ”€ ë¹„ë””ì˜¤ ì¬ìƒ (Component[0])
   â”‚     â†“ (íŠ¹ì • ì‹œì )
   â”‚
   â”œâ”€ ì§ˆë¬¸ í‘œì‹œ (Component[1])
   â”‚     â†“
   â”‚
   â”œâ”€ ìŒì„± ì…ë ¥ (Web Speech API - í´ë¼ì´ì–¸íŠ¸)
   â”‚     â†“
   â”‚
   â”œâ”€ POST /api/validate â”€â†’ [Backend]
   â”‚     { userSpeech, correctAnswer }
   â”‚                                    â”‚
   â”‚                                    â”œâ”€ ì •ë‹µ ë¹„êµ
   â”‚                                    â””â”€ Return { isCorrect, feedback }
   â”‚
   â””â”€ ê²°ê³¼ í‘œì‹œ
```

### 3ë‹¨ê³„ ì‹¤ì‹œê°„ ë©´ì ‘ í”Œë¡œìš°

```
[ì‚¬ìš©ì] â†’ [Frontend]
   â”‚
   â”œâ”€ í”„ë¡¬í”„íŠ¸ ì…ë ¥: "ë§ˆì¼€íŒ… ì¸í„´ ë©´ì ‘"
   â”‚     â†“
   â”‚
   â”œâ”€ POST /api/interview/start â”€â†’ [Backend]
   â”‚     { prompt: "ë§ˆì¼€íŒ… ì¸í„´ ë©´ì ‘" }
   â”‚                                    â”‚
   â”‚                                    â”œâ”€ GPT: ì´ˆê¸° ì§ˆë¬¸ ìƒì„±
   â”‚                                    â”‚   "ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”"
   â”‚                                    â”‚
   â”‚                                    â””â”€ WebSocket ì„¸ì…˜ ìƒì„±
   â”‚
   â”œâ”€ WebSocket ì—°ê²°
   â”‚     â†“
   â”‚
   â”œâ”€ [3ë¶„ ë£¨í”„ ì‹œì‘]
   â”‚   â”‚
   â”‚   â”œâ”€ ì§ˆë¬¸ ìˆ˜ì‹ : "ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”"
   â”‚   â”‚     â†“
   â”‚   â”‚
   â”‚   â”œâ”€ ìŒì„± ë‹µë³€ ë…¹ìŒ + ì „ì†¡
   â”‚   â”‚     â†“
   â”‚   â”‚
   â”‚   â”œâ”€ [Backend] â†’ [KoSpeech API]
   â”‚   â”‚                    POST /transcribe
   â”‚   â”‚                    { audio: blob }
   â”‚   â”‚                         â†“
   â”‚   â”‚                    Return "ì €ëŠ” ..."
   â”‚   â”‚
   â”‚   â”œâ”€ [Backend] â†’ [GPT API]
   â”‚   â”‚                    ë¶„ì„: í‚¤ì›Œë“œ ëˆ„ë½? ë¬¸ë§¥ ì–´ìƒ‰?
   â”‚   â”‚                    ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±: "ë§ˆì¼€íŒ… ê²½í—˜ì€?"
   â”‚   â”‚
   â”‚   â””â”€ ë°˜ë³µ (3ë¶„ ì¢…ë£Œê¹Œì§€)
   â”‚
   â”œâ”€ [3ë¶„ ì¢…ë£Œ]
   â”‚     â†“
   â”‚
   â””â”€ GET /api/interview/{sessionId}/feedback
                                    â”‚
                                    â””â”€ Return [
                                        { timeline: 30, type: "MISSING_KEYWORD", ... },
                                        { timeline: 120, type: "AWKWARD_CONTEXT", ... }
                                    ]
```

---

## ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­

### ì„±ëŠ¥
- ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°: 3ì´ˆ ì´ë‚´ ì´ˆê¸° ë¡œë”©
- ìŒì„±ì¸ì‹ ì‘ë‹µ: 1ì´ˆ ì´ë‚´ (í´ë¼ì´ì–¸íŠ¸), 3ì´ˆ ì´ë‚´ (ì„œë²„)
- API ì‘ë‹µì‹œê°„: í‰ê·  200ms ì´í•˜

### í™•ì¥ì„±
- ë™ì‹œ ì ‘ì†ì 1000ëª… ì§€ì›
- ë¹„ë””ì˜¤ CDNì„ í†µí•œ ê¸€ë¡œë²Œ ë°°í¬
- ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥í•œ ë¬´ìƒíƒœ(stateless) API

### ë³´ì•ˆ
- OAuth 2.0 ì†Œì…œ ë¡œê·¸ì¸
- JWT ê¸°ë°˜ ì¸ì¦
- HTTPS í•„ìˆ˜
- ë¹„ë””ì˜¤ URL ì‹œê°„ ì œí•œ ì„œëª… (Pre-signed URL)

### ê°€ìš©ì„±
- 99.9% ì—…íƒ€ì„ ëª©í‘œ
- AWS Multi-AZ ë°°í¬
- ìë™ ë°±ì—… (ì¼ 1íšŒ)

---

## ë°°í¬ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CloudFront CDN                   â”‚
â”‚  (ì •ì  íŒŒì¼ + ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   S3 Bucket      â”‚      â”‚  Route 53     â”‚
â”‚  (React Build)   â”‚      â”‚   (DNS)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  ALB (Load    â”‚
                          â”‚   Balancer)   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  EC2 Instance 1  â”‚  â”‚  EC2 Instance 2  â”‚  â”‚  EC2 Auto    â”‚
â”‚  (Spring Boot)   â”‚  â”‚  (Spring Boot)   â”‚  â”‚  Scaling     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  RDS PostgreSQL       â”‚
          â”‚  (Multi-AZ)           â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Separate EC2 (GPU Instance)              â”‚
â”‚  Python Flask (KoSpeech)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Docker ê¸°ë°˜ ë°°í¬ ì•„í‚¤í…ì²˜

### ì „ì²´ Docker Compose êµ¬ì¡°

```yaml
version: '3.8'

services:
  # Frontend
  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - VITE_API_URL=http://localhost:8080
    depends_on:
      - backend

  # Backend
  backend:
    build: ./backend
    ports:
      - "8080:8080"
    environment:
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/edutech
      - ML_STT_SERVICE_URL=http://ml-stt:5000
      - ML_EMOTION_SERVICE_URL=http://ml-emotion:5001
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - postgres
      - ml-stt
      - ml-emotion

  # ML Service 1: ìŒì„±ì¸ì‹ (KoSpeech)
  ml-stt:
    build:
      context: ./ml-services/stt
      dockerfile: Dockerfile.gpu  # ë˜ëŠ” Dockerfile.cpu
    ports:
      - "5000:5000"
    environment:
      - USE_GPU=true
      - MODEL_TYPE=kospeech  # ë˜ëŠ” whisper-api
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./models/kospeech:/app/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ML Service 2: ê°ì • ë¶„ì„ (DeepFace)
  ml-emotion:
    build:
      context: ./ml-services/emotion
      dockerfile: Dockerfile
    ports:
      - "5001:5001"
    environment:
      - MODEL_BACKEND=opencv  # 'opencv', 'ssd', 'retinaface'
      - DETECTION_BACKEND=opencv
    volumes:
      - ./models/deepface:/root/.deepface
    # CPUë¡œ ì¶©ë¶„íˆ ë¹ ë¦„ (GPU ë¶ˆí•„ìš”)

  # Database
  postgres:
    image: postgres:14
    environment:
      - POSTGRES_DB=edutech
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"

volumes:
  postgres-data:
```

### ë¡œì»¬ ê°œë°œ ì‹¤í–‰

```bash
# .env íŒŒì¼ ìƒì„±
echo "OPENAI_API_KEY=sk-..." > .env
echo "USE_GPU=true" >> .env

# Docker Compose ì‹¤í–‰ (ë¡œì»¬ RTX 3070 ì‚¬ìš©)
docker-compose up --build

# ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
# http://localhost:3000
```

---

## ML Service 2: DeepFace ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤

### ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     DeepFace Emotion Service            â”‚
â”‚          (Python Flask)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Flask API (Port 5001)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  DeepFace Library                â”‚  â”‚
â”‚  â”‚  - VGG-Face, FaceNet, OpenFace   â”‚  â”‚
â”‚  â”‚  - Emotion Detection (7 classes) â”‚  â”‚
â”‚  â”‚  - Age, Gender, Race             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OpenCV (Face Detection)         â”‚  â”‚
â”‚  â”‚  - Haar Cascade                  â”‚  â”‚
â”‚  â”‚  - DNN (SSD, RetinaFace)         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dockerfile

```dockerfile
# ml-services/emotion/Dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ëª¨ë¸ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ë¹Œë“œ ì‹œ)
RUN python3 -c "from deepface import DeepFace; \
    DeepFace.build_model('Emotion'); \
    print('DeepFace models downloaded')"

COPY . .

EXPOSE 5001

CMD ["python3", "app.py"]
```

### requirements.txt

```txt
# ml-services/emotion/requirements.txt
flask==3.0.0
deepface==0.0.79
opencv-python==4.8.1.78
numpy==1.24.3
pillow==10.1.0
tf-keras==2.15.0
```

### Flask API êµ¬í˜„

```python
# ml-services/emotion/app.py
from flask import Flask, request, jsonify
from deepface import DeepFace
import cv2
import numpy as np
import base64
from datetime import datetime
import os

app = Flask(__name__)

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
MODEL_BACKEND = os.getenv('MODEL_BACKEND', 'opencv')
DETECTION_BACKEND = os.getenv('DETECTION_BACKEND', 'opencv')

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy', 'service': 'emotion-analysis'})

@app.route('/analyze-frame', methods=['POST'])
def analyze_frame():
    """
    ë‹¨ì¼ í”„ë ˆì„ ê°ì • ë¶„ì„

    Request:
        - frame: ì´ë¯¸ì§€ íŒŒì¼ (jpeg/png)

    Response:
        {
            "emotion": "happy",
            "emotion_scores": {
                "angry": 0.01,
                "disgust": 0.0,
                "fear": 0.02,
                "happy": 0.85,
                "sad": 0.03,
                "surprise": 0.05,
                "neutral": 0.04
            },
            "dominant_emotion": "happy",
            "face_confidence": 0.95,
            "gaze_direction": "camera",
            "smile_intensity": 0.8,
            "face_region": {"x": 100, "y": 50, "w": 200, "h": 200},
            "timestamp": "2025-01-20T10:30:45"
        }
    """
    try:
        # ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°
        if 'frame' not in request.files:
            return jsonify({'error': 'No frame provided'}), 400

        file = request.files['frame']
        img_array = np.frombuffer(file.read(), np.uint8)
        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)

        if img is None:
            return jsonify({'error': 'Invalid image'}), 400

        # DeepFaceë¡œ ê°ì • ë¶„ì„
        result = DeepFace.analyze(
            img_path=img,
            actions=['emotion', 'age', 'gender'],
            enforce_detection=False,  # ì–¼êµ´ ì—†ì–´ë„ ì—ëŸ¬ ì•ˆë‚¨
            detector_backend=DETECTION_BACKEND
        )

        # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨ (ì—¬ëŸ¬ ì–¼êµ´ ê°€ëŠ¥)
        if isinstance(result, list):
            result = result[0]  # ì²« ë²ˆì§¸ ì–¼êµ´ë§Œ ì‚¬ìš©

        # ì‹œì„  ë°©í–¥ ë¶„ì„ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        face_region = result.get('region', {})
        gaze = analyze_gaze(img, face_region)

        # ë¯¸ì†Œ ê°•ë„ ê³„ì‚°
        smile_intensity = result['emotion'].get('happy', 0) / 100.0

        return jsonify({
            'emotion': result['dominant_emotion'],
            'emotion_scores': result['emotion'],
            'dominant_emotion': result['dominant_emotion'],
            'face_confidence': result.get('face_confidence', 1.0),
            'gaze_direction': gaze,
            'smile_intensity': smile_intensity,
            'face_region': face_region,
            'age': result.get('age'),
            'gender': result.get('dominant_gender'),
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500


def analyze_gaze(img, face_region):
    """
    ì‹œì„  ë°©í–¥ ë¶„ì„ (ê°„ë‹¨í•œ êµ¬í˜„)

    ì‹¤ì œë¡œëŠ” eye landmarks í•„ìš”í•˜ì§€ë§Œ,
    í•´ì»¤í†¤ìš©ìœ¼ë¡œëŠ” ì–¼êµ´ ìœ„ì¹˜ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
    """
    if not face_region:
        return 'unknown'

    img_center_x = img.shape[1] / 2
    face_center_x = face_region['x'] + face_region['w'] / 2

    # ì–¼êµ´ì´ ì´ë¯¸ì§€ ì¤‘ì•™ì— ìˆìœ¼ë©´ 'camera'
    if abs(face_center_x - img_center_x) < 100:
        return 'camera'
    elif face_center_x < img_center_x:
        return 'left'
    else:
        return 'right'


@app.route('/analyze-video', methods=['POST'])
def analyze_video():
    """
    ë¹„ë””ì˜¤ íŒŒì¼ ì „ì²´ ë¶„ì„ (ë³µê¸°ìš©)

    Request:
        - video: ë¹„ë””ì˜¤ íŒŒì¼ (.mp4, .webm)
        - interval: ë¶„ì„ ê°„ê²© (ì´ˆ, ê¸°ë³¸ 1ì´ˆ)

    Response:
        {
            "timeline": [
                {"time": 0, "emotion": "neutral", "confidence": 0.8},
                {"time": 1, "emotion": "happy", "confidence": 0.9},
                ...
            ],
            "summary": {
                "total_frames": 180,
                "avg_confidence": 0.85,
                "emotion_distribution": {
                    "happy": 60,
                    "neutral": 100,
                    "anxious": 20
                }
            }
        }
    """
    try:
        if 'video' not in request.files:
            return jsonify({'error': 'No video provided'}), 400

        video_file = request.files['video']
        interval = int(request.form.get('interval', 1))  # ê¸°ë³¸ 1ì´ˆ

        # ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_path = f'/tmp/video_{datetime.now().timestamp()}.mp4'
        video_file.save(temp_path)

        # ë¹„ë””ì˜¤ ë¶„ì„
        timeline = []
        emotion_counts = {}

        cap = cv2.VideoCapture(temp_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_interval = int(fps * interval)

        frame_idx = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # intervalë§ˆë‹¤ ë¶„ì„
            if frame_idx % frame_interval == 0:
                try:
                    result = DeepFace.analyze(
                        img_path=frame,
                        actions=['emotion'],
                        enforce_detection=False,
                        detector_backend=DETECTION_BACKEND
                    )

                    if isinstance(result, list):
                        result = result[0]

                    emotion = result['dominant_emotion']
                    time_sec = frame_idx / fps

                    timeline.append({
                        'time': round(time_sec, 1),
                        'emotion': emotion,
                        'confidence': max(result['emotion'].values()) / 100.0,
                        'scores': result['emotion']
                    })

                    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1

                except:
                    pass  # ì–¼êµ´ ê°ì§€ ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ

            frame_idx += 1

        cap.release()
        os.remove(temp_path)

        # ìš”ì•½ í†µê³„
        total_frames = len(timeline)
        avg_confidence = sum(t['confidence'] for t in timeline) / total_frames if total_frames > 0 else 0

        return jsonify({
            'timeline': timeline,
            'summary': {
                'total_frames': total_frames,
                'avg_confidence': round(avg_confidence, 2),
                'emotion_distribution': emotion_counts
            }
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/models/info', methods=['GET'])
def models_info():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ë³´"""
    return jsonify({
        'detector_backend': DETECTION_BACKEND,
        'model_backend': MODEL_BACKEND,
        'available_emotions': [
            'angry', 'disgust', 'fear', 'happy',
            'sad', 'surprise', 'neutral'
        ],
        'available_detectors': [
            'opencv', 'ssd', 'dlib', 'mtcnn',
            'retinaface', 'mediapipe'
        ]
    })


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001, debug=False)
```

### Spring Boot ì—°ë™

```java
// backend/src/main/java/com/seungjz/edutech/external/EmotionAnalysisClient.java
@Service
public class EmotionAnalysisClient {

    @Value("${ml.emotion.service.url}")
    private String emotionServiceUrl;

    private final RestTemplate restTemplate;

    public EmotionAnalysisClient(RestTemplate restTemplate) {
        this.restTemplate = restTemplate;
    }

    public EmotionAnalysisResult analyzeFrame(byte[] frameData) {
        String url = emotionServiceUrl + "/analyze-frame";

        // Multipart ìš”ì²­ ìƒì„±
        MultiValueMap<String, Object> body = new LinkedMultiValueMap<>();
        body.add("frame", new ByteArrayResource(frameData) {
            @Override
            public String getFilename() {
                return "frame.jpg";
            }
        });

        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.MULTIPART_FORM_DATA);

        HttpEntity<MultiValueMap<String, Object>> requestEntity =
            new HttpEntity<>(body, headers);

        ResponseEntity<EmotionAnalysisResult> response =
            restTemplate.postForEntity(url, requestEntity, EmotionAnalysisResult.class);

        return response.getBody();
    }

    @Data
    public static class EmotionAnalysisResult {
        private String emotion;
        private Map<String, Double> emotionScores;
        private String dominantEmotion;
        private Double faceConfidence;
        private String gazeDirection;
        private Double smileIntensity;
        private Map<String, Integer> faceRegion;
        private String timestamp;
    }
}
```

---

## ê°ì • ë¶„ì„ API ë¹„êµ

| API | ë¹„ìš© | ì •í™•ë„ | ì‘ë‹µì†ë„ | í”„ë¼ì´ë²„ì‹œ | í•´ì»¤í†¤ ì í•©ë„ |
|-----|------|--------|----------|------------|--------------|
| **DeepFace (ì˜¤í”ˆì†ŒìŠ¤)** | âœ… ë¬´ë£Œ | â­â­â­â­ (85-90%) | ë¹ ë¦„ (0.2ì´ˆ/í”„ë ˆì„) | âœ… ë¡œì»¬ ì²˜ë¦¬ | ğŸ† ìµœê³  |
| Azure Face API | 30,000ê±´/ì›” ë¬´ë£Œ | â­â­â­â­â­ (95%+) | ì¤‘ê°„ (0.5ì´ˆ) | âŒ í´ë¼ìš°ë“œ ì „ì†¡ | â­â­â­â­ |
| Google Cloud Vision | 1,000ê±´/ì›” ë¬´ë£Œ | â­â­â­â­â­ (95%+) | ì¤‘ê°„ (0.6ì´ˆ) | âŒ í´ë¼ìš°ë“œ ì „ì†¡ | â­â­â­ |
| AWS Rekognition | 5,000ê±´/ì›” (12ê°œì›”) | â­â­â­â­â­ (95%+) | ì¤‘ê°„ (0.5ì´ˆ) | âŒ í´ë¼ìš°ë“œ ì „ì†¡ | â­â­â­ |
| Face++ | 1,000ê±´/ì¼ ë¬´ë£Œ | â­â­â­â­ (90%) | ë¹ ë¦„ (0.3ì´ˆ) | âŒ í´ë¼ìš°ë“œ ì „ì†¡ | â­â­â­â­ |

### DeepFace ì„ íƒ ì´ìœ 

1. **ì™„ì „ ë¬´ë£Œ** - API í˜¸ì¶œ ì œí•œ ì—†ìŒ
2. **Docker í†µí•©** - ë¡œì»¬ GPU ì „ëµê³¼ ì™„ë²½ ë§¤ì¹­
3. **í”„ë¼ì´ë²„ì‹œ** - ë¹„ë””ì˜¤ê°€ ì™¸ë¶€ë¡œ ë‚˜ê°€ì§€ ì•ŠìŒ (GDPR ì¤€ìˆ˜)
4. **MIT ë¼ì´ì„¼ìŠ¤** - ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥
5. **ë¹ ë¥¸ ì‘ë‹µ** - ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì—†ìŒ
6. **ì˜¤í”„ë¼ì¸ ë™ì‘** - ì¸í„°ë„· ì—†ì–´ë„ ì‘ë™

### DeepFace ê°ì • í´ë˜ìŠ¤

```python
# 7ê°€ì§€ ê¸°ë³¸ ê°ì •
emotions = [
    'angry',     # í™”ë‚¨
    'disgust',   # í˜ì˜¤
    'fear',      # ë‘ë ¤ì›€
    'happy',     # í–‰ë³µ
    'sad',       # ìŠ¬í””
    'surprise',  # ë†€ëŒ
    'neutral'    # ì¤‘ë¦½
]
```

---

## ë¡œì»¬ â†’ í´ë¼ìš°ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### 1ë‹¨ê³„: ë¡œì»¬ ê°œë°œ (Day 1-2)

```bash
# RTX 3070 GPU ì‚¬ìš©
docker-compose up

# ì¥ì :
# - ë¹„ìš©: ì „ê¸°ì„¸ ~3,780ì›
# - í•™ìŠµ ê³¡ì„ : ì—†ìŒ
# - ê°œë°œ ì†ë„: ìµœê³ 
```

### 2ë‹¨ê³„: í´ë¼ìš°ë“œ ë°°í¬ (Day 3 - ë°ëª¨)

#### ì˜µì…˜ A: Whisper API ì „í™˜ (GPU ë¶ˆí•„ìš”)

```bash
# .env ìˆ˜ì •
USE_GPU=false
MODEL_TYPE=whisper-api

# Render.com/Railway ë¬´ë£Œ ë°°í¬
docker-compose -f docker-compose.prod.yml up
```

#### ì˜µì…˜ B: Azure Container Instances

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ
docker build -t yourrepo/ml-emotion:latest ./ml-services/emotion
docker push yourrepo/ml-emotion:latest

# Azure ë°°í¬
az container create \
  --resource-group edutech \
  --name emotion-service \
  --image yourrepo/ml-emotion:latest \
  --cpu 2 --memory 4 \
  --ports 5001

# STTëŠ” GPU í•„ìš” ì‹œ
az container create \
  --resource-group edutech \
  --name stt-service \
  --image yourrepo/ml-stt:latest \
  --gpu-count 1 \
  --gpu-sku K80
```

### 3ë‹¨ê³„: Kubernetes (í”„ë¡œë•ì…˜)

```yaml
# k8s/emotion-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: emotion-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: emotion-service
  template:
    metadata:
      labels:
        app: emotion-service
    spec:
      containers:
      - name: emotion
        image: yourrepo/ml-emotion:latest
        ports:
        - containerPort: 5001
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
```

---

## ê°œë°œ ë¡œë“œë§µ

### Phase 1: MVP (4ì£¼)
- [ ] ì†Œì…œ ë¡œê·¸ì¸ êµ¬í˜„
- [ ] 1ë‹¨ê³„ í•™ìŠµ ì»¨í…ì¸  ê´€ë¦¬
- [ ] ë¹„ë””ì˜¤ ì¬ìƒ + í´ë¼ì´ì–¸íŠ¸ ìŒì„±ì¸ì‹
- [ ] ê¸°ë³¸ ì •ë‹µ ê²€ì¦

### Phase 2: ê³ ê¸‰ ê¸°ëŠ¥ (4ì£¼)
- [ ] 2ë‹¨ê³„ ë“œë¼ë§ˆ ì»¨í…ì¸  ì¶”ê°€
- [ ] KoSpeech í†µí•©
- [ ] í•™ìŠµ ì§„ë„ ë° ë³µê¸° ê¸°ëŠ¥

### Phase 3: ì‹¤ì‹œê°„ ë©´ì ‘ (4ì£¼)
- [ ] 3ë‹¨ê³„ WebSocket êµ¬í˜„
- [ ] GPT ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
- [ ] íƒ€ì„ë¼ì¸ í”¼ë“œë°± ì‹œìŠ¤í…œ

### Phase 4: ìµœì í™” (2ì£¼)
- [ ] ì„±ëŠ¥ íŠœë‹
- [ ] CDN ì„¤ì •
- [ ] ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

---

## ê¸°ìˆ ì  ë„ì „ê³¼ì œ ë° í•´ê²° ë°©ì•ˆ

### 1. ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”
**ë¬¸ì œ**: ëŒ€ìš©ëŸ‰ MP4 íŒŒì¼ ì „ì†¡ ì‹œ ë¡œë”© ì‹œê°„ ì¦ê°€

**í•´ê²°**:
- HLS/DASH í”„ë¡œí† ì½œ ì ìš© (ì²­í¬ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë°)
- AWS CloudFront CDN í™œìš©
- Adaptive Bitrate Streaming (ë„¤íŠ¸ì›Œí¬ ìƒí™© ë”°ë¼ í™”ì§ˆ ì¡°ì •)

### 2. ìŒì„±ì¸ì‹ ì •í™•ë„
**ë¬¸ì œ**: ì™¸êµ­ì¸ ë°œìŒ, ë°°ê²½ ì†ŒìŒ ì²˜ë¦¬

**í•´ê²°**:
- KoSpeech ëª¨ë¸ + ë°œìŒ ë³´ì • í›„ì²˜ë¦¬
- ì‚¬ìš©ìë³„ ìŒì„± í”„ë¡œí•„ í•™ìŠµ (ì ì§„ì  ê°œì„ )
- ì†ŒìŒ ì œê±° ì „ì²˜ë¦¬ (WebRTC Audio Processing)

### 3. ì‹¤ì‹œê°„ ë©´ì ‘ ë™ì‹œì„±
**ë¬¸ì œ**: ë‹¤ìˆ˜ ì‚¬ìš©ì ë™ì‹œ ë©´ì ‘ ì‹œ ì„œë²„ ë¶€í•˜

**í•´ê²°**:
- WebSocket í’€ë§ (Netty ê¸°ë°˜)
- Redis Pub/Subìœ¼ë¡œ ì„¸ì…˜ ê´€ë¦¬
- ë¹„ë™ê¸° ìŒì„±ì¸ì‹ ì²˜ë¦¬ (Queue ê¸°ë°˜)

### 4. í´ë¼ì´ì–¸íŠ¸ vs ì„œë²„ ìŒì„±ì¸ì‹ ê²°ì •
**ìµœì¢… ê¶Œì¥ ì‚¬í•­**:

| ë‹¨ê³„ | ì²˜ë¦¬ ìœ„ì¹˜ | ì´ìœ  |
|------|----------|------|
| 1-2ë‹¨ê³„ | í´ë¼ì´ì–¸íŠ¸ (Web Speech API) | ì¦‰ê° í”¼ë“œë°±, ê°„ë‹¨í•œ ì •ë‹µ ë¹„êµ |
| 3ë‹¨ê³„ | ì„œë²„ (KoSpeech) | ë³µì¡í•œ ë¬¸ë§¥ ë¶„ì„, ì§€ì†ì  ëª¨ë¸ ê°œì„  |

---

## ì°¸ê³  ìë£Œ

### ìŒì„±ì¸ì‹
- [KoSpeech GitHub](https://github.com/sooftware/kospeech)
- [í•œêµ­ì–´ STT êµ¬í˜„ ê°€ì´ë“œ](https://velog.io/@letgodchan0/%EC%9D%8C%EC%84%B1%EC%9D%B8%EC%8B%9D-%ED%95%9C%EA%B5%AD%EC%96%B4-STT-5)
- [OpenAI Whisper API](https://platform.openai.com/docs/guides/speech-to-text)

### ê°ì • ë¶„ì„
- [DeepFace GitHub](https://github.com/serengil/deepface)
- [DeepFace ì‚¬ìš© ê°€ì´ë“œ](https://viso.ai/computer-vision/deepface/)
- [Facial Expression Recognition (FER)](https://github.com/topics/face-emotion-recognition)

### ë¹„ë””ì˜¤ ì²˜ë¦¬
- [MediaRecorder API MDN](https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder)
- [getUserMedia API](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia)
- [HLS Video Streaming Guide](https://docs.aws.amazon.com/ko_kr/mediaconvert/latest/ug/what-is.html)

### Docker & ë°°í¬
- [Docker Compose GPU Support](https://docs.docker.com/compose/gpu-support/)
- [Azure Container Instances](https://learn.microsoft.com/azure/container-instances/)
- [Kubernetes GPU Scheduling](https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/)
